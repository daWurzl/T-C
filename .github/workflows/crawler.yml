# ğŸ GitHub Actions Workflow zum AusfÃ¼hren des Crawlers

# ğŸ“› Anzeigename in der Actions-OberflÃ¤che
name: run crawler

# ğŸ“… Wann der Workflow ausgelÃ¶st wird
# In diesem Fall: manuell Ã¼ber "Run workflow"-Button
on:
  workflow_dispatch:

# ğŸ› ï¸ Definition des Jobs
jobs:
  run-crawler:
    # ğŸ“¦ Laufumgebung: GitHub verwendet eine Linux-Maschine mit Ubuntu
    runs-on: ubuntu-latest

    # ğŸ“œ Schritte, die der Job ausfÃ¼hrt
    steps:
      # ğŸ” Schritt 1: Repository auschecken (damit Code & Dateien verfÃ¼gbar sind)
      - name: Checkout Repo
        uses: actions/checkout@v4

      # ğŸ Schritt 2: Python installieren (fÃ¼r den Crawler)
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'  # WÃ¤hlt die neueste 3.x-Version

      # ğŸ“¦ Schritt 3: BenÃ¶tigte Bibliotheken installieren
      - name: Install dependencies
        run: |
          pip install requests beautifulsoup4

      # ğŸ§  Schritt 4: Crawler-Skript ausfÃ¼hren
      # Das Skript generiert eine neue `data.json` basierend auf Webseitenanalyse
      - name: Run crawler script
        run: python extract_info.py

      # ğŸ’¾ Schritt 5: Die neue `data.json` committen und pushen
      - name: Commit updated data.json
        run: |
          # Git-IdentitÃ¤t setzen (damit Git weiÃŸ, wer den Commit gemacht hat)
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # Ã„nderungen zur Commitliste hinzufÃ¼gen
          git add data.json

          # Commit-Befehl: Wenn keine Ã„nderungen â†’ Fehler vermeiden durch '|| echo "No changes"'
          git commit -m "Update data.json via crawler" || echo "No changes"

          # Ã„nderungen in das Repo zurÃ¼ckpushen
          git push
